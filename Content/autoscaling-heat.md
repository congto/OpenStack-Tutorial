###Scale an application with Heat
The Heat orchestration service can help with one of the most important topics in cloud computing: scalability. When a cloud application suffers for heavy load, there are two ways to scale:

1. **Vertical Scale.** This is the most obvious solution: when the server resources are not enough, use a more powerful server. Vertical scaling resizes the compute instance to a larger flavor, so that it gets more CPUs, more RAM and more disk space. This type of scaling works well up to a point. Once the maximum supported number of CPUs, RAM and disk space that an instance can have, there are no way to scale more. If the load generated by users continues to grow beyond that, we need to find a different way to scale.

2. **Horizontal Scale.** The other approach to scalability is to have the application running on a cluster of multiple servers. When an application runs on two or more instances, a load balancer is required to distributes client requests among servers. In this way, the application will be able to handle much larger volumes of clients. With this type of scaling, it is going to take longer to reach limits, since we continue to add servers behind the load balancer until we exhaust compute resources in the cloud. 

In this section, we are going to deploy an horizontal scaling stack by mean of Heat templates. Just to keep things simple, we are going to scale a simple web server providing a static page. We'll use two approach: manual scale and automatic scale (autoscaling).

####Manual Horizontal Scaling
Manual scaling requires the user scales the cluster manually when the load from clients reach the cluster limits. We start by writing a simple Heat template to deploy an Apache webserver on Ubuntu. The most clever part of the template is installing Apache in Ubuntu and customize the home page with the IP address of the server. Connecting to the Apache through the load balancer and refreshing the home page should show a changing IP address, because each time the page will be handled by a different server. We use the *cloudinit* capability to run an user data script at time of the instance start.

Here a snippet of ``cluster-heat-stack.yaml`` template:
```
resources:
  webserver:
    type: OS::Nova::Server
    properties:
      image: ubuntu
      flavor: small
      key_name: demokey
      networks:
        - network: { get_param: private_network }
      user_data: |
        #!/bin/bash
        apt-get install apache2 -y
        echo "Hello "$(hostname -I)"!" > /var/www/html/index.html
```

Then we need for a way to create multiple identical copies of the Apache webserver. Heat provides a resource type called ``OS::Heat::ResourceGroup``. This resource wraps any standard resource definition, like a compute server, and creates multiple identical copies of that resource. So, change the snippet above as following:
```
parameters:
  cluster_size:
    type: number
    label: Cluster size
    description: Number of webserver instances in the cluster.
    default: 2

resources:
  cluster:
    type: OS::Heat::ResourceGroup
    properties:
      count: { get_param: cluster_size }
      resource_def:
        type: OS::Nova::Server
        properties:
          image: ubuntu
          flavor: small
          key_name: demokey
          networks:
            - network: { get_param: private_network }
          user_data: |
            #!/bin/bash
            apt-get install apache2 -y
            echo "Hello "$(hostname -I)"!" > /var/www/html/index.html
```

The ``count`` property defines how many copies of the application to start. We set this value from a parameter ``cluster_size``, which the user can set. The ``resource_def`` property is where the resource that is getting scaled is configured.

Once we have the application running on multiple instances, we need for a load balancer that presents itself as the entry point to clients. The load balancer will accept the requests from clients and internally dispatch them to the actual servers.

Add the following to the template:
```
resources:
...
  loadbalancer:
    type: OS::Neutron::LoadBalancer
    properties:
      members: { get_attr: [cluster, refs] }
      pool_id: { get_resource: pool }
      protocol_port: { get_attr: [pool, vip, protocol_port] }

  pool:
    type: OS::Neutron::Pool
    properties:
      lb_method: ROUND_ROBIN
      protocol: HTTP
      subnet: { get_param: private_subnet }
      vip: { "protocol_port": 80 }
```

The ``loadbalancer`` resource creates a Load Balancer application based on the HAProxy relying on the OpenStack LBaaS Neutron plugin. Make sure to enable that pluging before to attemp to run the stack, see [LBaaS Configuration](./load-balancer.md). The Load Balancer takes the server list provided by the ResourceGroup cluster servers as its members.

Also the Load Balancer requires a Load Balancer ``pool`` resource where we specify:
* the load balance method (ROUND_ROBIN)
* the protocol (HTTP)
* the Virtual IP where it is listening for
* the port where it is listening for (80)

Note: when not specified, as in the above case, the Virtual IP address of the Load Balancer is automatically picked up from the subnet.


####Automatic Horizontal Scaling
